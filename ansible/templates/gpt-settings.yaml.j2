server:
  env_name: ${APP_ENV:local}

ui:
  enabled: true
  path: /
  default_chat_system_prompt: >
    Ты — Сайга, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.
  default_query_system_prompt: >
    Ответ должен быть развернутый с подробным пояснением как получился результат.
  delete_file_button_enabled: true
  delete_all_files_button_enabled: true

llm:
  mode: llamacpp
  # Should be matching the selected model
  max_new_tokens: 512
  context_window: 3900
  tokenizer: {{ llm_hf_tokenizer }}
  temperature: 0.1      # The temperature of the model. Increasing the temperature will make the model answer more creatively. A value of 0.1 would be more factual. (Default: 0.1)

llamacpp:
  prompt_style: "mistral"
  llm_hf_repo_id: {{ llm_hf_repo_id }}
  llm_hf_model_file: {{ llm_hf_model_file }}
  tfs_z: 1.0            # Tail free sampling is used to reduce the impact of less probable tokens from the output. A higher value (e.g., 2.0) will reduce the impact more, while a value of 1.0 disables this setting
  top_k: 40             # Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)
  top_p: 1.0            # Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)
  repeat_penalty: 1.1   # Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1)

embedding:
  mode: huggingface
  ingest_mode: parallel

huggingface:
  embedding_hf_model_name: {{ embedding_hf_model_name }}

vectorstore:
  database: qdrant

qdrant:
  path: local_data/private_gpt/qdrant
